{
  "name": "@agentic-sdlc/llm-service",
  "version": "1.0.0",
  "description": "Local LLM service with Llama 3.3 for all agents",
  "main": "src/llm-client.js",
  "scripts": {
    "start": "node src/llm-gateway.js",
    "dev": "nodemon src/llm-gateway.js",
    "test": "node examples/agent-integration.js",
    "docker:build": "docker build -t agentic-sdlc-llm-gateway .",
    "docker:run": "docker-compose -f docker-compose.llama.yml up",
    "docker:pull-models": "docker exec agentic-sdlc-llama-ollama sh -c 'ollama pull llama3.3:70b-instruct && ollama pull llama3.3:8b-instruct'",
    "health": "curl http://localhost:3458/health | jq"
  },
  "exports": {
    ".": {
      "import": "./src/llm-client.js",
      "require": "./src/llm-client.js"
    },
    "./client": "./src/llm-client.js",
    "./gateway": "./src/llm-gateway.js",
    "./examples": "./examples/agent-integration.js"
  },
  "dependencies": {
    "@anthropic-ai/sdk": "^0.24.0",
    "axios": "^1.6.0",
    "express": "^4.18.2",
    "ioredis": "^5.3.2"
  },
  "devDependencies": {
    "nodemon": "^3.0.1"
  },
  "keywords": [
    "llm",
    "llama",
    "ai",
    "agents",
    "local",
    "ollama",
    "vllm"
  ]
}